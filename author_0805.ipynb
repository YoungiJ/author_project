{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "> author분류\n",
    ">\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전 학습 모델의 사용은 금지이며\n",
    "가중치가 모두 초기화 된 모델의 '구조'만을 사용하여 제공된 데이터를 이용한 학습은 가능합니다.\n",
    "\n",
    "사전학습 된 BERT를 대회 데이터로 추가 학습 (fine-tuning)하지 않고 임베딩 레이어와 같이 사용\n",
    "\n",
    "bert가 안된다고 하신적이 없는 것같은데... fasttext나 glove에서 사전학습 임베딩 데이이터를 사용하는 건 되고,  bert를 사용하는 건 안된다고 명시된 것도 아닌데 그냥 임베딩 단계까지 알아서 사용하면 되는 것 아닐까요?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\miniconda3\\lib\\site-packages (from pyLDAvis) (1.10.1)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.8.5-cp39-cp39-win_amd64.whl (94 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Collecting funcy\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pyLDAvis) (1.2.2)\n",
      "Collecting numpy>=1.24.2\n",
      "  Downloading numpy-1.25.2-cp39-cp39-win_amd64.whl (15.6 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\miniconda3\\lib\\site-packages (from pyLDAvis) (61.2.0)\n",
      "Collecting pandas>=2.0.0\n",
      "  Downloading pandas-2.0.3-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.1.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.2)\n",
      "Installing collected packages: numpy, smart-open, pandas, numexpr, gensim, funcy, pyLDAvis\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: 'C:\\\\Users\\\\User\\\\miniconda3\\\\Lib\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 불러오기\n",
    "train = pd.read_csv('train.csv', encoding = 'utf-8')\n",
    "test = pd.read_csv('test_x.csv', encoding = 'utf-8')\n",
    "#sample_submission = pd.read_csv('open/sample_submission.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 3)\n",
      "(19617, 2)\n"
     ]
    }
   ],
   "source": [
    "train.head()\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop('index',axis=1)\n",
    "test=test.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  He was almost choking. There was so much, so m...       3\n",
       "1             “Your sister asked for it, I suppose?”       2\n",
       "2   She was engaged one day as she walked, in per...       1\n",
       "3  The captain was in the porch, keeping himself ...       4\n",
       "4  “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  “Not at all. I think she is one of the most ch...\n",
       "1  \"No,\" replied he, with sudden consciousness, \"...\n",
       "2  As the lady had stated her intention of scream...\n",
       "3  “And then suddenly in the silence I heard a so...\n",
       "4  His conviction remained unchanged. So far as I..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 전처리\n",
    "\n",
    "- preprocessing\n",
    "    1) 부호 제거 \n",
    "    2) 불용어 제거> stopwords removing\n",
    "    3) 형태소 분석> stemming ** 중요>> 토큰화\n",
    "    4) 표제어 추출> lemmatization\n",
    "- vectorization ** 정수 인코딩으로 변환\n",
    "    1) one-hot encoding\n",
    "    2) count vectorization\n",
    "    3) tf-idf\n",
    "    4) padding * 길이 조정\n",
    "   \n",
    "- embedding *벡터들을 특정 차원으로 임베딩 \n",
    "    1) word2vec\n",
    "    2) doc2vec\n",
    "   \n",
    "- modeling\n",
    "    1) GRU\n",
    "    2) LSTM\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1) 부호제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#부호를 제거해주는 함수\n",
    "def alpha_num(text):\n",
    "    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n",
    "\n",
    "train['text']=train['text'].apply(alpha_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        He was almost choking There was so much so muc...\n",
       "1                       Your sister asked for it I suppose\n",
       "2         She was engaged one day as she walked in peru...\n",
       "3        The captain was in the porch keeping himself c...\n",
       "4        Have mercy gentlemen odin flung up his hands D...\n",
       "                               ...                        \n",
       "54874    Is that you Mr Smith odin whispered I hardly d...\n",
       "54875    I told my plan to the captain and between us w...\n",
       "54876     Your sincere wellwisher friend and sister LUC...\n",
       "54877                 Then you wanted me to lend you money\n",
       "54878    It certainly had not occurred to me before but...\n",
       "Name: text, Length: 54879, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#부호가 사라진 것을 확인할 수 있다.\n",
    "train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting OktNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\user\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\user\\miniconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement oktapysdk (from okt) (from versions: none)\n",
      "ERROR: No matching distribution found for oktapysdk\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\user\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\user\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\user\\miniconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached okt-0.0.3-py3-none-any.whl (4.5 kB)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from Okt) (8.1.6)\n"
     ]
    }
   ],
   "source": [
    "pip install Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54879 entries, 0 to 54878\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    54879 non-null  object\n",
      " 1   author  54879 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 857.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "## 타입을 str으로 바꾸기\n",
    "\n",
    "train['text']=train['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 54879 entries, 0 to 54878\n",
      "Series name: text\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "54879 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 428.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train['text'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 단위로 문장 분리\n",
      "----------------------\n",
      "['데', '이콘', '에서', '다양한', '컴피티션', '을', '즐기면서', '실력', '있는', '데이터', '분석', '가로', '성장하세요', '!!.']\n",
      " \n",
      "문장에서 명사 추출\n",
      "----------------------\n",
      "['데', '이콘', '컴피티션', '실력', '데이터', '분석', '가로']\n",
      " \n",
      "품사 태킹(PoS)\n",
      "----------------------\n",
      "[('데', 'Noun'), ('이콘', 'Noun'), ('에서', 'Josa'), ('다양한', 'Adjective'), ('컴피티션', 'Noun'), ('을', 'Josa'), ('즐기면서', 'Verb'), ('실력', 'Noun'), ('있는', 'Adjective'), ('데이터', 'Noun'), ('분석', 'Noun'), ('가로', 'Noun'), ('성장하세요', 'Adjective'), ('!!.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# from konlpy.tag import Okt\n",
    "# Okt = Okt()\n",
    "\n",
    "# sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
    "\n",
    "# print(\"형태소 단위로 문장 분리\")\n",
    "# print(\"----------------------\")\n",
    "# print(Okt.morphs(sentence))\n",
    "# print(\" \")\n",
    "# print(\"문장에서 명사 추출\")\n",
    "# print(\"----------------------\")\n",
    "# print(Okt.nouns(sentence))\n",
    "# print(\" \")\n",
    "# print(\"품사 태킹(PoS)\")\n",
    "# print(\"----------------------\")\n",
    "# print(Okt.pos(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1-2) 불용어 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거해주는 함수\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stopwords:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "# 불용어\n",
    "stopwords = [ \"no\",\"not\",\"odin\",\"said\",\"will\",\"now\",\"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n",
    "             \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \n",
    "             \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \n",
    "             \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \n",
    "             \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \n",
    "             \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n",
    "             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \n",
    "             \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \n",
    "             \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \n",
    "             \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \n",
    "             \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 적용\n",
    "train['text'] = train['text'].str.lower()\n",
    "test['text'] = test['text'].str.lower()\n",
    "train['text'] = train['text'].apply(alpha_num).apply(remove_stopwords)\n",
    "test['text'] = test['text'].apply(alpha_num).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test 분리\n",
    "X_train = np.array([x for x in train['text']])\n",
    "X_test = np.array([x for x in test['text']])\n",
    "y_train = np.array([x for x in train['author']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['almost choking much much wanted say strange exclamations came lips pole gazed fixedly bundle notes hand looked odin evident perplexity',\n",
       "       'sister asked suppose',\n",
       "       'engaged one day walked perusing janes last letter dwelling passages proved jane not written spirits instead surprised mr odin saw looking odin meeting putting away letter immediately forcing smile said',\n",
       "       ..., 'sincere wellwisher friend sister lucy odin',\n",
       "       'wanted lend money', 'certainly not occurred said yes like'],\n",
       "      dtype='<U1433')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1-3) 형태소 분석\n",
    "- stemming ** 중요>> 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "author_list={}\n",
    "\n",
    "for idx,row_data in train.iterrows():\n",
    "    author=row_data['author']# 작가 번호 \n",
    "    setence=row_data['text']# text\n",
    "    \n",
    "    if author not in author_list:\n",
    "        author_list[author]=[]\n",
    "    author_list[author].append(setence)\n",
    "        \n",
    "\n",
    "author_list\n",
    "#tokenize해서 빈도를 시각화하기    \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "frequencies = {}\n",
    "\n",
    "for author,sentences in author_list.items():\n",
    "    all_words=[]\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)  # 문장을 단어로 토큰화\n",
    "        all_words+=words\n",
    "        \n",
    "    fdist=FreqDist(all_words)\n",
    "    frequencies[author] = fdist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: 3\n",
      "Most common words: [('odin', 11542), ('not', 5533), ('one', 2401), ('said', 2336), ('no', 2220), ('will', 1756), ('know', 1617), ('now', 1587), ('dont', 1506), ('come', 1366)]\n",
      "\n",
      "\n",
      "Author: 2\n",
      "Most common words: [('odin', 4625), ('not', 2651), ('said', 2473), ('upon', 2101), ('one', 1765), ('no', 1713), ('man', 1362), ('will', 1239), ('can', 1089), ('us', 1057)]\n",
      "\n",
      "\n",
      "Author: 1\n",
      "Most common words: [('odin', 8479), ('not', 5239), ('no', 1845), ('mr', 1742), ('said', 1491), ('will', 1356), ('mrs', 1330), ('odins', 1219), ('must', 1133), ('miss', 1082)]\n",
      "\n",
      "\n",
      "Author: 4\n",
      "Most common words: [('odin', 3981), ('said', 2388), ('not', 2124), ('upon', 1212), ('no', 1065), ('man', 1056), ('one', 1014), ('will', 893), ('now', 888), ('like', 775)]\n",
      "\n",
      "\n",
      "Author: 0\n",
      "Most common words: [('odin', 7509), ('said', 4755), ('mr', 3173), ('not', 2985), ('no', 1804), ('one', 1361), ('little', 1183), ('upon', 1137), ('know', 1096), ('now', 1041)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 작가별로 가장 많이 사용된 단어 출력\n",
    "\n",
    "for author, fdist in frequencies.items():\n",
    "    print(f\"Author: {author}\")\n",
    "    print(f\"Most common words: {fdist.most_common(10)}\")  # 상위 10개 단어 출력\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#빈도 객체를 데이터 프레임으로 변경\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = []\n",
    "for author, fdist in frequencies.items():\n",
    "    for word, freq in fdist.items():\n",
    "        data.append({'Author': author, 'Word': word, 'Frequency': freq})\n",
    "df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>odin</td>\n",
       "      <td>11542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40006</th>\n",
       "      <td>1</td>\n",
       "      <td>odin</td>\n",
       "      <td>8479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73014</th>\n",
       "      <td>0</td>\n",
       "      <td>odin</td>\n",
       "      <td>7509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>not</td>\n",
       "      <td>5533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>1</td>\n",
       "      <td>not</td>\n",
       "      <td>5239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72989</th>\n",
       "      <td>0</td>\n",
       "      <td>said</td>\n",
       "      <td>4755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19295</th>\n",
       "      <td>2</td>\n",
       "      <td>odin</td>\n",
       "      <td>4625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55307</th>\n",
       "      <td>4</td>\n",
       "      <td>odin</td>\n",
       "      <td>3981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73015</th>\n",
       "      <td>0</td>\n",
       "      <td>mr</td>\n",
       "      <td>3173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72985</th>\n",
       "      <td>0</td>\n",
       "      <td>not</td>\n",
       "      <td>2985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19222</th>\n",
       "      <td>2</td>\n",
       "      <td>not</td>\n",
       "      <td>2651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19087</th>\n",
       "      <td>2</td>\n",
       "      <td>said</td>\n",
       "      <td>2473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>3</td>\n",
       "      <td>one</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55324</th>\n",
       "      <td>4</td>\n",
       "      <td>said</td>\n",
       "      <td>2388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>3</td>\n",
       "      <td>said</td>\n",
       "      <td>2336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>2220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55327</th>\n",
       "      <td>4</td>\n",
       "      <td>not</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19101</th>\n",
       "      <td>2</td>\n",
       "      <td>upon</td>\n",
       "      <td>2101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40056</th>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>1845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73146</th>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>1804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Author  Word  Frequency\n",
       "16          3  odin      11542\n",
       "40006       1  odin       8479\n",
       "73014       0  odin       7509\n",
       "38          3   not       5533\n",
       "40000       1   not       5239\n",
       "72989       0  said       4755\n",
       "19295       2  odin       4625\n",
       "55307       4  odin       3981\n",
       "73015       0    mr       3173\n",
       "72985       0   not       2985\n",
       "19222       2   not       2651\n",
       "19087       2  said       2473\n",
       "64          3   one       2401\n",
       "55324       4  said       2388\n",
       "62          3  said       2336\n",
       "72          3    no       2220\n",
       "55327       4   not       2124\n",
       "19101       2  upon       2101\n",
       "40056       1    no       1845\n",
       "73146       0    no       1804"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df=df.sort_values(by='Frequency',ascending=False)\n",
    "sorted_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.barplot(x='Author', y='Frequency', hue='Word', data=df)\n",
    "# plt.legend(title='Word')\n",
    "# plt.title('Word Frequencies by Author')\n",
    "# plt.xlabel('Author')\n",
    "# plt.ylabel('Frequency')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4) n-gram (2, trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: ['I love', 'love natural', 'natural language', 'language processing']\n",
      "Trigrams: ['I love natural', 'love natural language', 'natural language processing']\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()  # 입력 텍스트를 단어로 분리\n",
    "    ngrams = []\n",
    "    \n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = ' '.join(words[i:i+n])  # n개의 단어를 연결하여 n-gram 생성\n",
    "        ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "input_text = \"I love natural language processing\"\n",
    "bigrams = generate_ngrams(input_text, 2)  # 바이그램 생성\n",
    "trigrams = generate_ngrams(input_text, 3)  # 트라이그램 생성\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파라미터 설정\n",
    "vocab_size = 20000\n",
    "embedding_dim = 16\n",
    "max_length = 500\n",
    "padding_type='post'\n",
    "#oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer에 fit\n",
    "tokenizer = Tokenizer(num_words = vocab_size)#, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터를 sequence로 변환해주고 padding 해줍니다.\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가벼운 NLP모델 생성\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 16)           320000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                408       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 125       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,533\n",
      "Trainable params: 320,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1372/1372 - 16s - loss: 1.5662 - accuracy: 0.2742 - val_loss: 1.5505 - val_accuracy: 0.2684 - 16s/epoch - 12ms/step\n",
      "Epoch 2/20\n",
      "1372/1372 - 15s - loss: 1.4357 - accuracy: 0.3854 - val_loss: 1.2980 - val_accuracy: 0.4463 - 15s/epoch - 11ms/step\n",
      "Epoch 3/20\n",
      "1372/1372 - 15s - loss: 1.1942 - accuracy: 0.4868 - val_loss: 1.1694 - val_accuracy: 0.4856 - 15s/epoch - 11ms/step\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'AssignAddVariableOp_3' defined at (most recent call last):\n    File \"c:\\Users\\User\\miniconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\User\\miniconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\User\\miniconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\User\\miniconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\User\\miniconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5100\\2037924028.py\", line 3, in <module>\n      history = model.fit(train_padded, y_train,\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1247, in apply_grad_to_update_var\n      return self._update_step(grad, var)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 240, in _update_step\n      self.update_step(gradient, variable)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\adam.py\", line 180, in update_step\n      v.assign_add(-v * (1 - self.beta_2))\nNode: 'AssignAddVariableOp_3'\nOOM when allocating tensor with shape[20000,16] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node AssignAddVariableOp_3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_962]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# fit model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m----> 3\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_padded, y_train, \n\u001b[0;32m      4\u001b[0m                     epochs\u001b[39m=\u001b[39;49mnum_epochs, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, \n\u001b[0;32m      5\u001b[0m                     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'AssignAddVariableOp_3' defined at (most recent call last):\n    File \"c:\\Users\\User\\miniconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\User\\miniconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\User\\miniconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\User\\miniconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\User\\miniconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5100\\2037924028.py\", line 3, in <module>\n      history = model.fit(train_padded, y_train,\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1247, in apply_grad_to_update_var\n      return self._update_step(grad, var)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 240, in _update_step\n      self.update_step(gradient, variable)\n    File \"c:\\Users\\User\\miniconda3\\lib\\site-packages\\keras\\optimizers\\adam.py\", line 180, in update_step\n      v.assign_add(-v * (1 - self.beta_2))\nNode: 'AssignAddVariableOp_3'\nOOM when allocating tensor with shape[20000,16] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node AssignAddVariableOp_3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_962]"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "num_epochs = 20\n",
    "history = model.fit(train_padded, y_train, \n",
    "                    epochs=num_epochs, verbose=2, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values\n",
    "pred = model.predict_proba(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "sample_submission[['0','1','2','3','4']] = pred\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_submission.to_csv('submission.csv', index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#0.58012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "embedded_size = 128# ?\n",
    "hidden_size = 150\n",
    "n_classes = 5\n",
    "EPOCHS = 7\n",
    "\n",
    "\n",
    "TEXT = data.Field(sequential=True,\n",
    "\n",
    "         use_vocab=True,\n",
    "\n",
    "         tokenize=str.split, \n",
    "\n",
    "         lower=True,\n",
    "\n",
    "         batch_first=True,\n",
    "\n",
    "         fix_length=None)\n",
    "\n",
    "\n",
    "\n",
    "LABEL = data.Field(sequential=False,\n",
    "\n",
    "          use_vocab=False,\n",
    "\n",
    "          is_target=True)\n",
    "\n",
    "\n",
    "\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "\n",
    "\n",
    "train_data = TabularDataset(\n",
    "\n",
    "  path = './data/train.nltk.csv',\n",
    "\n",
    "  format = 'csv',\n",
    "\n",
    "  fields = [('text', TEXT), ('label', LABEL)],\n",
    "\n",
    "  skip_header = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data = TabularDataset(\n",
    "\n",
    "  path = './data/test_x.nltk.1.csv',\n",
    "\n",
    "  format = 'csv',\n",
    "\n",
    "  fields = [('text', TEXT)],\n",
    "\n",
    "  skip_header = True)\n",
    "\n",
    "\n",
    "\n",
    "print(vars(train_data[0]))\n",
    "\n",
    "TEXT.build_vocab(train_data, min_freq=7, max_size=20000, vectors = \"fasttext.en.300d\")\n",
    "\n",
    "train_data, valid_data = train_data.split(split_ratio=0.8, stratified=True)\n",
    "\n",
    "\n",
    "\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "\n",
    "\n",
    "train_loader, valid_loader = BucketIterator.splits(\n",
    "\n",
    "  (train_data, valid_data),\n",
    "\n",
    "  batch_size = batch_size,\n",
    "\n",
    "  device='cuda:0',\n",
    "\n",
    "  shuffle = True,\n",
    "\n",
    "  sort_key=lambda x:len(x.text),\n",
    "\n",
    "  sort_within_batch=True\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_loader = BucketIterator(\n",
    "\n",
    "  test_data,\n",
    "\n",
    "  batch_size = batch_size,\n",
    "\n",
    "  device='cuda:0',\n",
    "\n",
    "  shuffle = False,\n",
    "\n",
    "  sort_key=lambda x:len(x.text),\n",
    "\n",
    "  sort_within_batch=True\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = LSTM(len(TEXT.vocab), embedded_size, hidden_size, n_classes).to('cuda:0')\n",
    "\n",
    "\n",
    "\n",
    "crit = nn.NLLLoss().to('cuda:0')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_loss = 999999\n",
    "\n",
    "best_model = None\n",
    "\n",
    "\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  train_accuracy = 0\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for x, y in tqdm(train_loader):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, y = x.to('cuda:0'), y.to('cuda:0')\n",
    "\n",
    "\n",
    "\n",
    "    y_hat = model(x)\n",
    "\n",
    "    loss = crit(y_hat, y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor):\n",
    "\n",
    "      accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
    "\n",
    "    else:\n",
    "\n",
    "      accuracy = 0\n",
    "\n",
    "    train_loss += float(loss) / len(train_loader)\n",
    "\n",
    "    train_accuracy += accuracy / len(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "  valid_loss = 0\n",
    "\n",
    "  valid_accuracy = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for x, y in valid_loader:\n",
    "\n",
    "      x, y = x.to('cuda:0'), y.to('cuda:0')\n",
    "\n",
    "      y_hat = model(x)\n",
    "\n",
    "      loss = crit(y_hat, y)\n",
    "\n",
    "       \n",
    "\n",
    "      if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor):\n",
    "\n",
    "        accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
    "\n",
    "      else:\n",
    "\n",
    "        accuracy = 0\n",
    "\n",
    "      valid_loss += float(loss) / len(valid_loader)\n",
    "\n",
    "      valid_accuracy += accuracy / len(valid_loader)\n",
    "\n",
    "       \n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "\n",
    "      best_loss = valid_loss\n",
    "\n",
    "      best_model = deepcopy(model.state_dict())\n",
    "\n",
    "       \n",
    "\n",
    "       \n",
    "\n",
    "  print(\"EPOCHS: {:2d} | train_accuracy: {:.4f} / train_loss: {:.4f} / valid_accuracy: {:.4f} / valid_loss: {:.4f} / best_loss: {:.4f}\".format\n",
    "\n",
    "       (i+1, train_accuracy, train_loss, valid_accuracy, valid_loss, best_loss))\n",
    "\n",
    "\n",
    "\n",
    "test_model = LSTM(len(TEXT.vocab), embedded_size, hidden_size, n_classes).to('cuda:0')\n",
    "\n",
    "test_model.load_state_dict(best_model)\n",
    "\n",
    "\n",
    "\n",
    "test_model.eval()\n",
    "\n",
    "y_hats = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "  for x_batch in test_loader:\n",
    "\n",
    "    x = x_batch.text.to('cuda:0')\n",
    "\n",
    "    y_hat = test_model(x).cpu()\n",
    "\n",
    "    y_hats += y_hat\n",
    "\n",
    "  y_hats = torch.stack(y_hats).exp()\n",
    "\n",
    "test_pred = y_hats.numpy()\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sub = pd.read_csv('./data/sample_submission.csv', index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "sub[sub.columns] = test_pred\n",
    "\n",
    "sub.head()\n",
    "\n",
    "sub.to_csv('./data/submission2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
